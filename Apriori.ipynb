{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7501 entries, 0 to 7500\n",
      "Columns: 120 entries,  asparagus to zucchini\n",
      "dtypes: int64(120)\n",
      "memory usage: 6.9 MB\n",
      "None\n",
      "(7501, 120)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"transactions_binarized.csv\")\n",
    "print(df.info())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing each item from the header of the data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_list = df.columns.tolist()\n",
    "items_dict = dict()\n",
    "\n",
    "for i, item in enumerate(item_list):\n",
    "    items_dict[item] = i + 1\n",
    "\n",
    "# display the most frequent items\n",
    "item_counts = df.sum(axis=0)\n",
    "item_counts = item_counts.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the transactions from the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2, 3, 5, 34, 40, 49, 54, 55, 61, 66, 73, 82, 92, 93, 98, 103, 107, 112, 115, 118}\n",
      "{16, 70, 38}\n",
      "{28}\n",
      "{5, 111}\n",
      "{39, 72, 73, 117, 55}\n"
     ]
    }
   ],
   "source": [
    "transactions = list()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    transaction = set()\n",
    "\n",
    "    for item in items_dict:\n",
    "        if row[item] == 1:\n",
    "            transaction.add(items_dict[item])\n",
    "    transactions.append(transaction)\n",
    "\n",
    "# print the first 5 transactions\n",
    "for i in range(5):\n",
    "    print(transactions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_support** function evaluates the support value for a set given all the transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def get_frequency(transactions, item_set):\n",
    "    match_count = float(0)\n",
    "    if len(transactions) == 0:\n",
    "        return match_count\n",
    "    for transaction in transactions:\n",
    "        if item_set.issubset(transaction):\n",
    "            match_count += 1\n",
    "\n",
    "    return match_count / len(transactions)\n",
    "\n",
    "\n",
    "def test_get_frequency():\n",
    "    # Test case 1: Empty transactions\n",
    "    transactions_empty = []\n",
    "    item_set_1 = {1, 2, 3}\n",
    "    expected_output_1 = 0.0\n",
    "    assert get_frequency(transactions_empty, item_set_1) == expected_output_1\n",
    "\n",
    "    # Test case 2: No matching transactions\n",
    "    transactions_no_match = [{4, 5}, {6, 7}, {8, 9}]\n",
    "    item_set_2 = {1, 2, 3}\n",
    "    expected_output_2 = 0.0\n",
    "    assert get_frequency(transactions_no_match,\n",
    "                         item_set_2) == expected_output_2\n",
    "\n",
    "    # Test case 3: Some matching transactions\n",
    "    transactions_some_match = [{1, 2, 3}, {2, 3, 4}, {3, 4, 5}, {1, 3, 5}]\n",
    "    item_set_3 = {1, 2, 3}\n",
    "    expected_output_3 = 0.25  # 1 out of 4 transactions\n",
    "    assert get_frequency(transactions_some_match,\n",
    "                         item_set_3) == expected_output_3\n",
    "\n",
    "    # Test case 4: All transactions matching\n",
    "    transactions_all_match = [{1, 2, 3}, {1, 2, 3}, {1, 2, 3}]\n",
    "    item_set_4 = {1, 2, 3}\n",
    "    expected_output_4 = 1.0  # All transactions match\n",
    "    assert get_frequency(transactions_all_match,\n",
    "                         item_set_4) == expected_output_4\n",
    "\n",
    "    # Test case 4: Subset of transactions matching\n",
    "    transactions_all_match = [{1, 2, 3, 4, 5}, {1, 2, 3}, {7, 8, 9}]\n",
    "    item_set_5 = {1, 2, 3}\n",
    "    expected_output_5 = float(2/3)\n",
    "    assert get_frequency(transactions_all_match,\n",
    "                         item_set_5) == expected_output_5\n",
    "\n",
    "    print(\"All tests passed!\")\n",
    "\n",
    "\n",
    "# Run the test cases\n",
    "test_get_frequency()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**generate_candidate_item_sets** function generates the candidate item sets of size k from the frequent item sets of size k-1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def generate_candidate_item_sets(frequent_item_sets, level):\n",
    "    current_level_candidates = list()\n",
    "\n",
    "    if len(frequent_item_sets[level - 1]) == 0:\n",
    "        return current_level_candidates\n",
    "\n",
    "    # Extract unique items from the frequent item sets of the previous level\n",
    "    unique_items = set()\n",
    "    for item_set, _ in frequent_item_sets[level - 1]:\n",
    "        unique_items.update(item_set)\n",
    "\n",
    "    # Generate candidates by combining unique items\n",
    "    for candidate_set in combinations(unique_items, level):\n",
    "        candidate_set = set(candidate_set)\n",
    "        current_level_candidates.append(candidate_set)\n",
    "\n",
    "    return current_level_candidates\n",
    "\n",
    "\n",
    "def test_generate_candidate_item_sets():\n",
    "    # Sample input data\n",
    "    frequent_item_sets = {\n",
    "        1: [({1}, 0.2), ({2}, 0.3), ({3}, 0.4)],\n",
    "        2: [({1, 2}, 0.1), ({1, 3}, 0.2), ({2, 3}, 0.3)],\n",
    "    }\n",
    "    level = 3\n",
    "\n",
    "    # Expected output\n",
    "    expected_output = [{1, 2, 3}]\n",
    "\n",
    "    # Call the function\n",
    "    result = generate_candidate_item_sets(frequent_item_sets, level)\n",
    "\n",
    "    # Assertion\n",
    "    assert (\n",
    "        result == expected_output\n",
    "    ), f\"Test failed: Expected {expected_output}, but got {result}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "test_generate_candidate_item_sets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**pruning** function prunes the candidate sets evaluated after completing the self-join part. For each itemset, it finds all its subsets by dropping a single elements from it and checks if that subset was present in the previous level or not. If that subset was not present in the previous level, then the current set is not valid and must not be used, and is thus pruned.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cases for get_single_drop_subsets passed!\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 106\u001b[0m\n\u001b[0;32m    104\u001b[0m test_get_single_drop_subsets()\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# test_is_valid_set()\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m \u001b[43mtest_pruning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[98], line 87\u001b[0m, in \u001b[0;36mtest_pruning\u001b[1;34m()\u001b[0m\n\u001b[0;32m     85\u001b[0m candidate_set_valid \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}, {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m}, {\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m}]\n\u001b[0;32m     86\u001b[0m expected_output_valid \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}, {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m}, {\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m}]\n\u001b[1;32m---> 87\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m     88\u001b[0m     pruning(frequent_item_sets, level, candidate_set_valid)\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;241m==\u001b[39m expected_output_valid\n\u001b[0;32m     90\u001b[0m )\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# Test case 3: Candidate set with invalid item sets\u001b[39;00m\n\u001b[0;32m     93\u001b[0m candidate_set_invalid \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}, {\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m}, {\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m}]\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_single_drop_subsets(item_set):\n",
    "    single_drop_subsets = list()\n",
    "    for item in item_set:\n",
    "        temp = item_set.copy()\n",
    "        temp.remove(item)\n",
    "        single_drop_subsets.append(temp)\n",
    "\n",
    "    return single_drop_subsets\n",
    "\n",
    "\n",
    "def test_get_single_drop_subsets():\n",
    "    # Test case 1: Empty item set\n",
    "    item_set_empty = set()\n",
    "    expected_output_empty = []\n",
    "    assert get_single_drop_subsets(item_set_empty) == expected_output_empty\n",
    "\n",
    "    # Test case 2: Non-empty item set\n",
    "    item_set_non_empty = {1, 2, 3}\n",
    "    expected_output_non_empty = [{2, 3}, {1, 3}, {1, 2}]\n",
    "    assert get_single_drop_subsets(\n",
    "        item_set_non_empty) == expected_output_non_empty\n",
    "\n",
    "    print(\"Test cases for get_single_drop_subsets passed!\")\n",
    "\n",
    "\n",
    "def is_valid_set(item_set, prev_level_sets):\n",
    "    single_drop_subsets = get_single_drop_subsets(item_set)\n",
    "\n",
    "    for single_drop_set in single_drop_subsets:\n",
    "        if single_drop_set not in prev_level_sets:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_is_valid_set():\n",
    "    # # Test case 1: Empty previous level sets\n",
    "    # item_set = {1, 2}\n",
    "    # prev_level_sets_empty = []\n",
    "    # assert is_valid_set(item_set, prev_level_sets_empty) is True\n",
    "\n",
    "    # Test case 2: Item set not present in previous level sets\n",
    "    item_set_not_present = {1, 2}\n",
    "    prev_level_sets = [{3, 4}, {5, 6}]\n",
    "    assert is_valid_set(item_set_not_present, prev_level_sets) is False\n",
    "\n",
    "    # Test case 3: Item set present in previous level sets\n",
    "    item_set_present = {1, 2}\n",
    "    prev_level_sets_present = [{1, 2}, {3, 4}]\n",
    "    assert is_valid_set(item_set_present, prev_level_sets_present) is True\n",
    "\n",
    "    print(\"Test cases for is_valid_set passed!\")\n",
    "\n",
    "\n",
    "def prune_candidates(frequent_item_sets, level, candidate_set):\n",
    "    post_prune_candidates_set = list()\n",
    "    if len(candidate_set) == 0:\n",
    "        return post_prune_candidates_set\n",
    "\n",
    "    prev_level_sets = list()\n",
    "    for item_set, _ in frequent_item_sets[level - 1]:\n",
    "        prev_level_sets.append(item_set)\n",
    "\n",
    "    for item_set in candidate_set:\n",
    "        if is_valid_set(item_set, prev_level_sets):\n",
    "            post_prune_candidates_set.append(item_set)\n",
    "\n",
    "    return post_prune_candidates_set\n",
    "\n",
    "\n",
    "def test_prune_candidates():\n",
    "    # Test case 1: Empty candidate set\n",
    "    frequent_item_sets = {\n",
    "        1: [({1}, 0.2), ({2}, 0.3)],\n",
    "        2: [({1, 2}, 0.1), ({1, 3}, 0.2)],\n",
    "    }\n",
    "    level = 3\n",
    "    candidate_set_empty = []\n",
    "    expected_output_empty = []\n",
    "    assert (\n",
    "        prune_candidates(frequent_item_sets, level, candidate_set_empty)\n",
    "        == expected_output_empty\n",
    "    )\n",
    "\n",
    "    # Test case 2: Candidate set with valid item sets\n",
    "    candidate_set_valid = [{1, 2, 3}, {2, 3, 4}, {3, 4, 5}]\n",
    "    expected_output_valid = [{1, 2, 3}, {2, 3, 4}, {3, 4, 5}]\n",
    "    assert (\n",
    "        prune_candidates(frequent_item_sets, level, candidate_set_valid)\n",
    "        == expected_output_valid\n",
    "    )\n",
    "\n",
    "    # Test case 3: Candidate set with invalid item sets\n",
    "    candidate_set_invalid = [{1, 2}, {2, 3}, {4, 5}]\n",
    "    expected_output_invalid = []\n",
    "    assert (\n",
    "        prune_candidates(frequent_item_sets, level, candidate_set_invalid)\n",
    "        == expected_output_invalid\n",
    "    )\n",
    "\n",
    "    print(\"Test cases for pruning passed!\")\n",
    "\n",
    "\n",
    "# Run the test cases\n",
    "test_get_single_drop_subsets()\n",
    "# test_is_valid_set()\n",
    "test_prune_candidates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main apriori algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def apriori(min_support):\n",
    "    frequent_item_sets = defaultdict(list)\n",
    "\n",
    "    print(\"level : 1\", end=\" \")\n",
    "\n",
    "    for item in range(1, len(item_list) + 1):\n",
    "        support = get_frequency(transactions, {item})\n",
    "        if support >= min_support:\n",
    "            frequent_item_sets[1].append(({item}, support))\n",
    "\n",
    "    for level in range(2, len(item_list) + 1):\n",
    "        print(level, end=\" \")\n",
    "        current_level_candidates = generate_candidate_item_sets(\n",
    "            frequent_item_sets, level\n",
    "        )\n",
    "        post_pruning_candidates = prune_candidates(\n",
    "            frequent_item_sets, level, current_level_candidates\n",
    "        )\n",
    "\n",
    "        if len(post_pruning_candidates) == 0:\n",
    "            print(f\"{level} reached, no more frequent item sets.\")\n",
    "            break\n",
    "\n",
    "        for item_set in post_pruning_candidates:\n",
    "            support = get_frequency(transactions, item_set)\n",
    "            if support >= min_support:\n",
    "                frequent_item_sets[level].append((item_set, support))\n",
    "\n",
    "    return frequent_item_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the **minimum support** value here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_support = 0.005\n",
    "frequent_item_sets = apriori(transactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug print statements to check the number of frequent sets calculated for each level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in frequent_item_sets:\n",
    "    print(len(frequent_item_sets[level]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug statement to check the frequent sets calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for level in frequent_item_sets:\n",
    "    print(frequent_item_sets[level])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Generating Association Rules\n",
    "\n",
    "Prepare input for calculating association rules: Create a dictionary of each frequent itemset against its support value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_support_dict = dict()\n",
    "item_list = list()\n",
    "\n",
    "key_list = list(items_dict.keys())\n",
    "val_list = list(items_dict.values())\n",
    "\n",
    "for level in frequent_item_sets:\n",
    "    for set_support_pair in frequent_item_sets[level]:\n",
    "        for i in set_support_pair[0]:\n",
    "            item_list.append(key_list[val_list.index(i)])\n",
    "        item_support_dict[frozenset(item_list)] = set_support_pair[1]\n",
    "        item_list = list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debug statement to check the values in the dictionary created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_support_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Function\n",
    "\n",
    "**find_subset** finds all the subsets of the given itemset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subset(item, item_length):\n",
    "    combs = []\n",
    "    for i in range(1, item_length + 1):\n",
    "        combs.append(list(combinations(item, i)))\n",
    "\n",
    "    subsets = []\n",
    "    for comb in combs:\n",
    "        for elt in comb:\n",
    "            subsets.append(elt)\n",
    "\n",
    "    return subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**association_rules** generates the association rules in accordance with the given _minimum confidence_ value and the provided dictionary of itemsets against their support values. For itemsets of more than one element, it first finds all their subsets. For every subset A, it calculates the set B = itemset-A. If B is not empty, the confidence of B is calculated. If this value is more than _minimum confidence_ value, the rule _A->B_ is added to the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def association_rules(min_confidence, support_dict):\n",
    "    rules = list()\n",
    "    for item, support in support_dict.items():\n",
    "        item_length = len(item)\n",
    "\n",
    "        if item_length > 1:\n",
    "            subsets = find_subset(item, item_length)\n",
    "\n",
    "            for A in subsets:\n",
    "                B = item.difference(A)\n",
    "\n",
    "                if B:\n",
    "                    A = frozenset(A)\n",
    "\n",
    "                    AB = A | B\n",
    "\n",
    "                    confidence = support_dict[AB] / support_dict[A]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((A, B, confidence))\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Minimum confidence value here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = association_rules(\n",
    "    min_confidence=0.5, support_dict=item_support_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Printing the output in the required format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of rules: \", len(association_rules), \"\\n\")\n",
    "\n",
    "for rule in association_rules:\n",
    "    print(\n",
    "        '{0} -> {1} <confidence: {2}>'.format(set(rule[0]), set(rule[1]), rule[2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
